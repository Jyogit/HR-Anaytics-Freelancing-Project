{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT PYTHON & SKLEARN PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python basic packages \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import boto\n",
    "import math\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USER INPUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for training dataset\n",
    "train_csv_path = 'C:/Python/ClientJob/train.csv'\n",
    "# Path for testing dataset\n",
    "test_csv_path = 'C:/Python/ClientJob/test.csv'\n",
    "\n",
    "# Path for pushing output as csv \n",
    "train_out_csv = 'C:/Python/ClientJob/train_predicted.csv'\n",
    "test_out_csv = 'C:/Python/ClientJob/test_predicted.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE MAPPING DICTIONARY \n",
    "\n",
    "##### Standardize mapping dictionary useful to convert categorical variables into dummy variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cntryid = {'Italy':'00', 'Japan':'01', 'Russian Federation':'02', 'Germany':'03', 'United States':'04','France':'05', 'Spain':'06', 'Korea':'07', 'Canada':'08',\n",
    "                'Slovak Republic':'09', 'United Kingdom':'10', 'Netherlands':'11', 'Greece':'12', 'Turkey':'13', 'Poland':'14', 'Finland':'15', 'Slovenia':'16','Israel':'17',\n",
    "                'Belgium':'18', 'New Zealand':'19', 'Lithuania':'20', 'Norway':'21', 'Chile':'22', 'Sweden':'23', 'Denmark':'24', 'Singapore':'25', \n",
    "                'Czech Republic':'26', 'Estonia':'27','Austria':'28', 'Ireland':'29', 'Cyprus':'30'}\n",
    "\n",
    "dict_ctryrgn = {'North America and Western Europe':'00', 'East Asia and the Pacific (richer countries)':'01', 'Central and Eastern Europe':'02', \n",
    "                'Latin America and the Caribbean':'03','Oceania':'04'}\n",
    "\n",
    "dict_edlevel3 = {'Low':'00', 'Medium':'01', 'High':'02'}\n",
    "\n",
    "dict_gender_r ={'Male':0,'Female':1}\n",
    "\n",
    "dict_yesno = {'No':0, 'Yes':1}\n",
    "\n",
    "dict_incpr ={'Less than 10':'00', '10 to less than 25':'01', '25 to less than 50':'02', '50 to less than 75':'03', '75 to less than 90':'04', '90 or more':'05'}\n",
    "\n",
    "dict_nativelang = {'Test language same as native language':0, 'Test language not same as native language':1}\n",
    "\n",
    "dict_iscoskil4 = {'Semi-skilled blue-collar occupations':'00','Skilled occupations':'01', 'Semi-skilled white-collar occupations':'02',\n",
    "                  'Elementary occupations':'03'}\n",
    "\n",
    "dict_ageg5 = {'Aged 16-19':'00', 'Aged 20-24':'01', 'Aged 25-29':'02', 'Aged 30-34':'03', 'Aged 35-39':'04', 'Aged 40-44':'05', 'Aged 45-49':'06', \n",
    "              'Aged 50-54':'07', 'Aged 55-59':'08', 'Aged 60-65':'09'}\n",
    "\n",
    "dict_ageg10 = {'24 or less':'00', '25-34':'01', '35-44':'02', '45-54':'03', '55 plus':'04'}\n",
    "\n",
    "dict_neet = {'Employed or participated in education or training in last 12 months':1, \n",
    "             'Not currently employed and did not participate in education or training in last 12 months (NEET)':0}\n",
    "\n",
    "dict_wle_ca = {'All zero response':'00', 'Lowest to 20%':'01','More than 20% to 40%':'02', 'More than 40% to 60%':'03', 'More than 60% to 80%':'04','More than 80%':'05'}\n",
    "\n",
    "dict_v31 = {'Teacher training and education science':'00', 'Social sciences, business and law':'01', 'Services':'02','Science, mathematics and computing':'03', \n",
    "            'General programmes':'04', 'Engineering, manufacturing and construction':'05', 'Humanities, languages and arts':'06', 'Health and welfare':'07', \n",
    "            'Agriculture and veterinary':'08'}\n",
    "\n",
    "dict_earnbnsdcl = {'Lowest decile':'00', '2nd decile':'01', '3rd decile':'02', '4th decile':'03', '5th decile':'04', '6th decile':'05', '7th decile':'06', \n",
    "                   '8th decile':'07', '9th decile':'08', 'Highest decile':'09'}\n",
    "\n",
    "dict_v151 = {'Aged 15 or younger':'00',  'Aged 16-19':'01',  'Aged 20-24':'02', 'Aged 25-29':'03',   'Aged 30-34':'04', 'Aged 35 or older':'05'}\n",
    "\n",
    "dict_edwork = {'In work only':'00', 'Not in education or work and has not participated in education or training in last 12 months (NEET)':'01', 'In education only':'02', 'Not in education or work but has participated in education or training in last 12 months':'03', 'In education and work':'04'}\n",
    "\n",
    "dict_learnsrvy = {'Never':'00','Less than once a month':'01','Less than once a week but at least once a month':'02', 'At least once a week but not every day':'03',\n",
    "                  'Every day':'04'}\n",
    "\n",
    "dict_satfsrvy = {'Extremely dissatisfied':'00','Dissatisfied':'01','Neither satisfied nor dissatisfied':'02','Satisfied':'03','Extremely satisfied':'04'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA TRANSFORMATION LIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables \n",
    "base_data_col = ['job_performance','cntryid','ctryrgn','edwork','edlevel3','gender_r','computerexperience','nativespeaker','nativelang','yearlyincpr','iscoskil4','ageg5lfs',\n",
    "                'ageg10lfs','neet','readytolearn_wle_ca','planning_wle_ca','readhome_wle_ca','readwork_wle_ca','writhome_wle_ca','writwork_wle_ca','taskdisc_wle_ca',\n",
    "                'learnatwork_wle_ca','ictwork_wle_ca','icthome_wle_ca','influence_wle_ca','v31','earnhrbonusdcl',\n",
    "                 'v151','v235','v246','v214','v276','v181','v60','v90','v157','v74']\n",
    "\n",
    "#Imputing Cat variables \n",
    "cat_var_lst = ['cntryid','ctryrgn','edwork','edlevel3','gender_r','computerexperience','nativespeaker','nativelang','yearlyincpr','iscoskil4','ageg5lfs','ageg10lfs','neet',\n",
    "            'readytolearn_wle_ca','planning_wle_ca','readhome_wle_ca','readwork_wle_ca','writhome_wle_ca','writwork_wle_ca', 'taskdisc_wle_ca', 'learnatwork_wle_ca',\n",
    "               'ictwork_wle_ca', 'icthome_wle_ca','influence_wle_ca','v31','earnhrbonusdcl','v151','v235','v246','v214','v276','v181','v60','v90','v157','v74']\n",
    "\n",
    "num_imp_lst = ['v235']\n",
    "\n",
    "dummy_var_lst = ['cntryid', 'ctryrgn', 'edwork','edlevel3','yearlyincpr','iscoskil4','ageg10lfs','ageg5lfs', 'readytolearn_wle_ca','planning_wle_ca',\n",
    "                 'readhome_wle_ca',\n",
    "                'readwork_wle_ca','writhome_wle_ca','writwork_wle_ca','taskdisc_wle_ca', 'learnatwork_wle_ca','ictwork_wle_ca','icthome_wle_ca','influence_wle_ca',\n",
    "                 'earnhrbonusdcl', 'v31','v151','v246','v214','v276','v181']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA TRANSFORMATION FUNCTION & MODEL SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ReadCsv File\n",
    "def readCsvFile(csv_file_path):\n",
    "  # Read csv file \n",
    "  df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "  # Cleaning Country Gen col\n",
    "  df['ctryrgn'] = np.where(df['cntryid']=='New Zealand', 'Oceania', df['ctryrgn'])\n",
    "  print(df.shape)\n",
    "  return df\n",
    "\n",
    "# Function to Impute categorical variables:Fill missing values with the most frequent value\n",
    "def imputeCatVar(impute_df, cat_var_lst):\n",
    "  for col in cat_var_lst:\n",
    "    impute_df[col].fillna(impute_df[col].value_counts().idxmax(), inplace=True)\n",
    "  return impute_df\n",
    "\n",
    "# Function to Impute numerical variables :Fill missing values with the most frequent value\n",
    "def imputerNumMedian(df, num_imp_lst):\n",
    "  for col in num_imp_lst:\n",
    "    median_value = df[col].median()\n",
    "    df[col] = df[col].fillna(median_value).astype(int)\n",
    "  return df\n",
    "\n",
    "# Function to standardize numerical variables using sklearn MinMaxScaler\n",
    "def numScaling(df, num_imp_lst):\n",
    "  data_df = df.copy()\n",
    "  scale_df = data_df[num_imp_lst]\n",
    "  data_df = data_df.drop(data_df[num_imp_lst],axis=1)\n",
    "  numeric_scaler = MinMaxScaler(feature_range=(0,1), copy=True)\n",
    "  numeric_scaler.fit(scale_df)\n",
    "  scale_df = pd.DataFrame(numeric_scaler.transform(scale_df), index=scale_df.index, columns=scale_df.columns)\n",
    "  data_df = pd.concat([data_df, scale_df], axis=1)\n",
    "  return data_df\n",
    "\n",
    "# Function to create dummy variables for all categorical variables \n",
    "def dummyCatFeature(df, dummy_var_lst):\n",
    "  data_df = df.copy()\n",
    "  for col in dummy_var_lst:\n",
    "    dummy_df = pd.get_dummies(data_df[col], prefix=col)\n",
    "    data_df = pd.concat([data_df, dummy_df], axis=1)\n",
    "  return data_df\n",
    "\n",
    "# Function created to MapValues from Dctionary defination \n",
    "def mapDictValue(map_df):\n",
    "  # Mapping Sring column\n",
    "  map_df['cntryid'] = map_df['cntryid'].map(dict_cntryid).astype(str) \n",
    "  map_df['ctryrgn'] = map_df['ctryrgn'].map(dict_ctryrgn).astype(str)   \n",
    "  map_df['iscoskil4'] = map_df['iscoskil4'].map(dict_iscoskil4).astype(str) \n",
    "  map_df['edwork'] = map_df['edwork'].map(dict_edwork).astype(str)    \n",
    "  map_df['yearlyincpr'] = map_df['yearlyincpr'].map(dict_incpr).astype(str)\n",
    "  map_df['nativelang'] = map_df['nativelang'].map(dict_nativelang).astype(str)\n",
    "  map_df['ageg10lfs'] = map_df['ageg10lfs'].map(dict_ageg10).astype(str)\n",
    "  map_df['ageg5lfs'] = map_df['ageg5lfs'].map(dict_ageg5).astype(str)\n",
    "  map_df['readytolearn_wle_ca'] = map_df['readytolearn_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['planning_wle_ca'] = map_df['planning_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['readhome_wle_ca'] = map_df['readhome_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['readwork_wle_ca'] = map_df['readwork_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['writhome_wle_ca'] = map_df['writhome_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['writwork_wle_ca'] = map_df['writwork_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['taskdisc_wle_ca'] = map_df['taskdisc_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['learnatwork_wle_ca'] = map_df['learnatwork_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['ictwork_wle_ca'] = map_df['ictwork_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['icthome_wle_ca'] = map_df['icthome_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['influence_wle_ca'] = map_df['influence_wle_ca'].map(dict_wle_ca).astype(str)\n",
    "  map_df['earnhrbonusdcl'] = map_df['earnhrbonusdcl'].map(dict_earnbnsdcl).astype(str)\n",
    "  map_df['v31'] = map_df['v31'].map(dict_v31).astype(str)\n",
    "  map_df['v151'] = map_df['v151'].map(dict_v151).astype(str)  \n",
    "  map_df['v246'] = map_df['v246'].map(dict_learnsrvy).astype(str)\n",
    "  map_df['v214'] = map_df['v214'].map(dict_learnsrvy).astype(str)\n",
    "  map_df['v276'] = map_df['v276'].map(dict_learnsrvy).astype(str)\n",
    "  map_df['v181'] = map_df['v181'].map(dict_satfsrvy).astype(str)\n",
    "  map_df['edlevel3'] = map_df['edlevel3'].map(dict_edlevel3).astype(str)\n",
    "  \n",
    "  # Mapping Integer column    \n",
    "  map_df['gender_r'] = map_df['gender_r'].map(dict_gender_r).astype(int)\n",
    "  map_df['computerexperience'] = map_df['computerexperience'].map(dict_yesno).astype(int)    \n",
    "  map_df['nativespeaker'] = map_df['nativespeaker'].map(dict_yesno).astype(int)        \n",
    "  map_df['neet'] = map_df['neet'].map(dict_neet).astype(int)\n",
    "  map_df['v60'] = map_df['v60'].map(dict_yesno).astype(int)  \n",
    "  map_df['v90'] = map_df['v90'].map(dict_yesno).astype(int)    \n",
    "  map_df['v157'] = map_df['v157'].map(dict_yesno).astype(int)  \n",
    "  map_df['v74'] = map_df['v74'].map(dict_yesno).astype(int)      \n",
    "  \n",
    "  return map_df\n",
    "  \n",
    "# Function to display Score, Mean Score and Standard Deviation\n",
    "def display_scores(scores):   \n",
    "  scores = np.array(scores).astype(float).round(decimals=3)\n",
    "  print(\"Scores: {}\".format(scores))\n",
    "  print(\"Mean: {:.3f}\".format(scores.mean()))\n",
    "  print(\"Standard deviation: {:.4f}\".format(scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURES FOR CHAMPION MODEL  \n",
    "Feature Selection: Using multiple feature selection procedures such as SelectKBest and Recursive Feature Elimination \n",
    "\n",
    "'edlevel', 'ageg5lf', 'cntryid', 'computerexperience', 'ctryrgn', 'earnhrbonusdcl', 'edwork', 'gender_r', 'icthome_wle_ca', 'ictwork_wle_ca', 'influence_wle', 'iscoskil', 'learnatwork_wle_ca', 'nativespeaker', 'neet', 'planning_wle_ca', 'readhome_wle_ca', 'readwork_wle_ca', 'readytolearn_wle_ca', 'taskdisc_wle_ca', 'v151', 'v157', 'v181', 'v214', 'v235', 'v246',\n",
    "'v276', 'v74', 'v90', 'writhome_wle_ca', 'writwork_wle_ca', 'yearlyincpr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmfrst_champ_feature_var = ['ageg5lfs_01','ageg5lfs_02','ageg5lfs_03','ageg5lfs_04','ageg5lfs_05','ageg5lfs_06','ageg5lfs_07','ageg5lfs_08','ageg5lfs_09',\n",
    "                    'cntryid_01','cntryid_02','cntryid_03','cntryid_04','cntryid_05','cntryid_06','cntryid_07','cntryid_08','cntryid_09','cntryid_10','cntryid_11',\n",
    "                    'cntryid_12','cntryid_13','cntryid_14','cntryid_15','cntryid_17','cntryid_18','cntryid_19','cntryid_21','cntryid_22','cntryid_23','cntryid_24',\n",
    "                    'cntryid_25','cntryid_26','cntryid_27','cntryid_28','cntryid_29',\n",
    "                    'computerexperience',\n",
    "                    'ctryrgn_01','ctryrgn_02','ctryrgn_03','ctryrgn_04',\n",
    "                    'earnhrbonusdcl_01','earnhrbonusdcl_02','earnhrbonusdcl_03','earnhrbonusdcl_04','earnhrbonusdcl_05','earnhrbonusdcl_06','earnhrbonusdcl_07',\n",
    "                    'earnhrbonusdcl_08','earnhrbonusdcl_09',\n",
    "                    'edlevel3','edwork_01','edwork_02','edwork_03','edwork_04',\n",
    "                    'gender_r',\n",
    "                    'icthome_wle_ca_01','icthome_wle_ca_02','icthome_wle_ca_03','icthome_wle_ca_04','icthome_wle_ca_05',\n",
    "                    'ictwork_wle_ca_01','ictwork_wle_ca_02','ictwork_wle_ca_03','ictwork_wle_ca_04','ictwork_wle_ca_05',\n",
    "                    'influence_wle_ca_01','influence_wle_ca_02','influence_wle_ca_03','influence_wle_ca_04','influence_wle_ca_05',\n",
    "                    'iscoskil4_01','iscoskil4_02','iscoskil4_03',\n",
    "                    'learnatwork_wle_ca_01','learnatwork_wle_ca_02','learnatwork_wle_ca_03','learnatwork_wle_ca_04','learnatwork_wle_ca_05',\n",
    "                    'nativespeaker',\n",
    "                    'neet',\n",
    "                    'planning_wle_ca_01','planning_wle_ca_02','planning_wle_ca_03','planning_wle_ca_04','planning_wle_ca_05',\n",
    "                    'readhome_wle_ca_01','readhome_wle_ca_02','readhome_wle_ca_03','readhome_wle_ca_04','readhome_wle_ca_05',\n",
    "                    'readwork_wle_ca_01','readwork_wle_ca_02','readwork_wle_ca_03','readwork_wle_ca_04','readwork_wle_ca_05',\n",
    "                    'readytolearn_wle_ca_01','readytolearn_wle_ca_02','readytolearn_wle_ca_03','readytolearn_wle_ca_04','readytolearn_wle_ca_05',\n",
    "                    'taskdisc_wle_ca_01','taskdisc_wle_ca_02','taskdisc_wle_ca_03','taskdisc_wle_ca_04','taskdisc_wle_ca_05',\n",
    "                    'v151_01','v151_02','v151_03','v151_04','v151_05',\n",
    "                    'v157',\n",
    "                    'v181_01','v181_02','v181_03','v181_04',\n",
    "                    'v214_01','v214_02','v214_03','v214_04',\n",
    "                    'v235',\n",
    "                    'v246_01','v246_02','v246_03','v246_04',\n",
    "                    'v276_01','v276_02','v276_03','v276_04',\n",
    "                    'v60',\n",
    "                    'v74',\n",
    "                    'v90',\n",
    "                    'writhome_wle_ca_01','writhome_wle_ca_02','writhome_wle_ca_03','writhome_wle_ca_04','writhome_wle_ca_05',\n",
    "                    'writwork_wle_ca_01','writwork_wle_ca_02','writwork_wle_ca_03','writwork_wle_ca_04','writwork_wle_ca_05',\n",
    "                    'yearlyincpr_01','yearlyincpr_02','yearlyincpr_03','yearlyincpr_04','yearlyincpr_05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPARE TRAIN & TEST DATA FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 380)\n"
     ]
    }
   ],
   "source": [
    "# Read Csv file \n",
    "df_train = readCsvFile(train_csv_path)\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "df_thresh = df_train[df_train.columns[df_train.isnull().mean() < 0.7]].copy()\n",
    "#Keeping the required columns \n",
    "df_filterd = df_thresh[base_data_col].copy()\n",
    "# Impute categorical variables with the Mod \n",
    "df_cat_imp = imputeCatVar(df_filterd, cat_var_lst)\n",
    "# Impute numerical variables with the mdeian value \n",
    "df_train_imp = imputerNumMedian(df_cat_imp, num_imp_lst)\n",
    "# Scaling numerical variables \n",
    "df_train_scl = numScaling(df_train_imp, num_imp_lst)\n",
    "# Mapping categorical and numerical variables\n",
    "df_train_map = mapDictValue(df_train_scl)\n",
    "# Create dummy indicator on categorical variables \n",
    "df_train_dmy = dummyCatFeature(df_train_map, dummy_var_lst)\n",
    "# Cleaned data for modelling\n",
    "cln_train_df = df_train_dmy.copy()\n",
    "\n",
    "# Scaling response variables as scores will be capped at 5000  \n",
    "#cln_train_df['job_performance'] = cln_train_df.apply(lambda row: math.log(((row.job_performance/5000)/(1-(row.job_performance/5000)))), axis = 1) \n",
    "\n",
    "# Stratified Shuffle Split on country and gender to create train and test data for modelling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(cln_train_df, cln_train_df[['cntryid','gender_r']]):\n",
    "    strat_train_set = cln_train_df.loc[train_index]\n",
    "    strat_test_set = cln_train_df.loc[test_index]\n",
    "    \n",
    "# Full train dataframe \n",
    "X_full_train_df = cln_train_df[rmfrst_champ_feature_var]\n",
    "y_full_train_df = cln_train_df['job_performance'].astype(int)\n",
    "\n",
    "# Convert Full train dataframe to array\n",
    "X_full_train = X_full_train_df.values\n",
    "y_full_train = y_full_train_df.values.ravel().astype(int)\n",
    "\n",
    "# Stratified Shuffled train dataframe  \n",
    "X_train_df = strat_train_set[rmfrst_champ_feature_var]\n",
    "y_train_df = strat_train_set['job_performance'].astype(int)\n",
    "\n",
    "# Stratified Shuffled test dataframe  \n",
    "X_test_df = strat_test_set[rmfrst_champ_feature_var]\n",
    "y_test_df = strat_test_set['job_performance'].astype(int)\n",
    "\n",
    "# Convert Shuffled train dataframe to arrays\n",
    "X_train = X_train_df.values\n",
    "y_train = y_train_df.values.ravel().astype(int)\n",
    "\n",
    "# Convert Shuffled test dataframe to arrays\n",
    "X_test = X_test_df.values\n",
    "y_test = y_test_df.values.ravel().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHAMPION MODEL: RANDOMFOREST\n",
    "\n",
    "~ Choosing Champion Model : LinearRegression, SGDRegressor, RandomForestRegressor, SVR and XGBoost.  RandomForestRegressor gave us the least MSE compared to different models trained.\n",
    "\n",
    "~ Cross Validation: Tested model stability using cross_val_score\n",
    "\n",
    "~ Hyper Parameter Tuning: Used Grid Search to tune Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Fitting Random Forest model on X_train and testing it on X_test dataset:\n",
      "-----------------------------------------------------------------------------\n",
      "Mean Squared Error on Test data: 28915.700\n",
      "\n",
      "--------------------------------------------------\n",
      "Cross validation scores on X_full_data\n",
      "--------------------------------------------------\n",
      "Cross validation Scores: [-32990.83883405 -32815.93270388 -34025.74292395 -30483.91492652\n",
      " -34495.33444169]\n",
      "Cross validation Mean: -32962.353\n",
      "Cross validation Standard deviation: 1388.9374\n"
     ]
    }
   ],
   "source": [
    "# *****************************************************************************************\n",
    "# Random Forest Regressor \n",
    "# *****************************************************************************************\n",
    "Rmforest = RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
    "                      max_features=20, max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                      n_jobs=None, oob_score=False, random_state=42,\n",
    "                      verbose=0, warm_start=False)\n",
    "\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print('Fitting Random Forest model on X_train and testing it on X_test dataset:')\n",
    "print('-----------------------------------------------------------------------------')\n",
    "\n",
    "# Train the model on training data\n",
    "Rmforest.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred = Rmforest.predict(X_test)\n",
    "print('Mean Squared Error on Test data: {:.3f}'.format(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('')\n",
    "print('--------------------------------------------------')\n",
    "print('Cross validation scores on X_full_data')    \n",
    "print('--------------------------------------------------')\n",
    "scores = cross_val_score(Rmforest, X_full_train, y_full_train, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "print(\"Cross validation Scores: {}\".format(scores))\n",
    "print(\"Cross validation Mean: {:.3f}\".format(scores.mean()))\n",
    "print(\"Cross validation Standard deviation: {:.4f}\".format(scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUNNING MODEL ON TEST DATSET AND GENERATE CSV FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 380)\n",
      "count test data :20000\n",
      "count test data :20000\n",
      "shape of final data :(20000, 381)\n"
     ]
    }
   ],
   "source": [
    "# Read Csv file \n",
    "df_test = readCsvFile(test_csv_path)\n",
    "#Keeping the required columns \n",
    "df_filterd = df_test[base_data_col].copy()\n",
    "# Impute categorical variables with the Mod \n",
    "df_cat_imp = imputeCatVar(df_filterd, cat_var_lst)\n",
    "# Impute numerical variables with the mdeian value \n",
    "df_test_imp = imputerNumMedian(df_cat_imp, num_imp_lst)\n",
    "# Scaling numerical variables \n",
    "df_test_scl = numScaling(df_test_imp, num_imp_lst)\n",
    "# Mapping categorical and numerical variables\n",
    "df_test_map = mapDictValue(df_test_scl)\n",
    "# Create dummy indicator on categorical variables \n",
    "df_test_dmy = dummyCatFeature(df_test_map, dummy_var_lst)\n",
    "# Cleaned data for modelling\n",
    "cln_test_df = df_test_dmy.copy()\n",
    "\n",
    "# Test dataframe \n",
    "X_test_df = cln_test_df[rmfrst_champ_feature_var]\n",
    "\n",
    "# Convert test dataframe to array\n",
    "X_test_values = X_test_df.values\n",
    "print('count test data :{}'.format(len(X_test_df)))\n",
    "\n",
    "# Predict Test data\n",
    "modle_prediction = Rmforest.predict(X_test_values)\n",
    "yhat = modle_prediction\n",
    "\n",
    "print('count test data :{}'.format(len(yhat)))\n",
    "y_pred = pd.DataFrame({ 'Predicted_JobPerformance': yhat}).astype(int)\n",
    "finalfor = pd.concat([df_test, y_pred], axis=1)\n",
    "print('shape of final data :{}'.format(finalfor.shape))\n",
    "\n",
    "# Export test csv file \n",
    "finalfor.to_csv(test_out_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CROSS CHECK JOB PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 380)\n",
      "count test data :20000\n",
      "count test data :20000\n",
      "shape of final data :(20000, 381)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_performance</th>\n",
       "      <th>Predict_JobPerformance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3164</td>\n",
       "      <td>3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2673</td>\n",
       "      <td>2671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2701</td>\n",
       "      <td>2703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2289</td>\n",
       "      <td>2285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2349</td>\n",
       "      <td>2349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3233</td>\n",
       "      <td>2836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1550</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3421</td>\n",
       "      <td>3421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3628</td>\n",
       "      <td>3628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2677</td>\n",
       "      <td>2678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2664</td>\n",
       "      <td>2666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3037</td>\n",
       "      <td>3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3302</td>\n",
       "      <td>3303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3003</td>\n",
       "      <td>3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3399</td>\n",
       "      <td>3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3341</td>\n",
       "      <td>3338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2149</td>\n",
       "      <td>2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2753</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2061</td>\n",
       "      <td>2061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2817</td>\n",
       "      <td>2817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2669</td>\n",
       "      <td>2669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2135</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3591</td>\n",
       "      <td>3584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3209</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2757</td>\n",
       "      <td>2758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2087</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2738</td>\n",
       "      <td>2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2618</td>\n",
       "      <td>2618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2966</td>\n",
       "      <td>2966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3322</td>\n",
       "      <td>3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>2769</td>\n",
       "      <td>2767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>2539</td>\n",
       "      <td>2547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>2804</td>\n",
       "      <td>2804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>3025</td>\n",
       "      <td>3028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>2455</td>\n",
       "      <td>2456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>3047</td>\n",
       "      <td>3051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>3838</td>\n",
       "      <td>3834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>3491</td>\n",
       "      <td>3484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>3299</td>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>3111</td>\n",
       "      <td>3111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>2672</td>\n",
       "      <td>2674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>2624</td>\n",
       "      <td>2623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>3052</td>\n",
       "      <td>3051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>3132</td>\n",
       "      <td>3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>2250</td>\n",
       "      <td>2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>3068</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>2923</td>\n",
       "      <td>2923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>3310</td>\n",
       "      <td>3309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>3231</td>\n",
       "      <td>3231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>3387</td>\n",
       "      <td>3389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>3298</td>\n",
       "      <td>3295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>2920</td>\n",
       "      <td>2919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>2998</td>\n",
       "      <td>2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>2829</td>\n",
       "      <td>2829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>3344</td>\n",
       "      <td>3345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>3224</td>\n",
       "      <td>3225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>3117</td>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>2725</td>\n",
       "      <td>3108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_performance  Predict_JobPerformance\n",
       "0                 3164                    3164\n",
       "1                 2673                    2671\n",
       "2                 2701                    2703\n",
       "3                 2289                    2285\n",
       "4                 2349                    2349\n",
       "5                 3233                    2836\n",
       "6                 1550                    1545\n",
       "7                 3421                    3421\n",
       "8                 3628                    3628\n",
       "9                 2677                    2678\n",
       "10                2664                    2666\n",
       "11                3037                    3036\n",
       "12                3302                    3303\n",
       "13                3003                    3005\n",
       "14                3399                    3402\n",
       "15                3341                    3338\n",
       "16                2149                    2145\n",
       "17                2753                    2756\n",
       "18                2061                    2061\n",
       "19                2817                    2817\n",
       "20                2669                    2669\n",
       "21                2135                    2142\n",
       "22                3591                    3584\n",
       "23                3209                    3208\n",
       "24                2757                    2758\n",
       "25                2087                    2087\n",
       "26                2738                    2736\n",
       "27                2618                    2618\n",
       "28                2966                    2966\n",
       "29                3322                    3322\n",
       "...                ...                     ...\n",
       "19970             2769                    2767\n",
       "19971             2539                    2547\n",
       "19972             2804                    2804\n",
       "19973             3025                    3028\n",
       "19974             2455                    2456\n",
       "19975             3047                    3051\n",
       "19976             3838                    3834\n",
       "19977             3491                    3484\n",
       "19978             2908                    2910\n",
       "19979             3299                    3299\n",
       "19980             3111                    3111\n",
       "19981             2672                    2674\n",
       "19982             2624                    2623\n",
       "19983             3052                    3051\n",
       "19984             3132                    3135\n",
       "19985             2250                    2248\n",
       "19986             3068                    3071\n",
       "19987             2923                    2923\n",
       "19988             3310                    3309\n",
       "19989             3231                    3231\n",
       "19990             3387                    3389\n",
       "19991             3298                    3295\n",
       "19992             2920                    2919\n",
       "19993             2998                    2996\n",
       "19994             2829                    2829\n",
       "19995             2754                    2754\n",
       "19996             3344                    3345\n",
       "19997             3224                    3225\n",
       "19998             3117                    3118\n",
       "19999             2725                    3108\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Csv file \n",
    "df_train = readCsvFile(train_csv_path)\n",
    "#Keeping the required columns \n",
    "df_filterd = df_train[base_data_col].copy()\n",
    "# Impute categorical variables with the Mod \n",
    "df_cat_imp = imputeCatVar(df_filterd, cat_var_lst)\n",
    "# Impute numerical variables with the mdeian value \n",
    "df_train_imp = imputerNumMedian(df_cat_imp, num_imp_lst)\n",
    "# Scaling numerical variables \n",
    "df_train_scl = numScaling(df_train_imp, num_imp_lst)\n",
    "# Mapping categorical and numerical variables\n",
    "df_train_map = mapDictValue(df_train_scl)\n",
    "# Create dummy indicator on categorical variables \n",
    "df_train_dmy = dummyCatFeature(df_train_map, dummy_var_lst)\n",
    "# Cleaned data for modelling\n",
    "cln_train_df = df_train_dmy.copy()\n",
    "\n",
    "# Train dataframe \n",
    "X_train_df = cln_train_df[rmfrst_champ_feature_var]\n",
    "\n",
    "# Convert train dataframe to array\n",
    "X_train_values = X_train_df.values\n",
    "print('count test data :{}'.format(len(X_train_values)))\n",
    "\n",
    "# Predict Test data\n",
    "modle_prediction = Rmforest.predict(X_train_values)\n",
    "yhat =modle_prediction\n",
    "\n",
    "print('count test data :{}'.format(len(yhat)))\n",
    "\n",
    "y_pred = pd.DataFrame({ 'Predict_JobPerformance': yhat}).astype(int)\n",
    "finaltrain = pd.concat([df_train, y_pred], axis=1).copy()\n",
    "print('shape of final data :{}'.format(finaltrain.shape))\n",
    "\n",
    "finaltrain[['job_performance','Predict_JobPerformance']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
